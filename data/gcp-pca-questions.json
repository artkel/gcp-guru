[
  {
    "question_number": 1,
    "question_text": "The development team has submitted a ticket for a web application that is crashing due to high CPU utilization. The network admin has suggested creating a Managed Instance Group to handle the load. Which suggested configuration below could solve the problem?",
    "answers": {
      "a": {
        "answer_text": "Create a Managed Instance Group with an autoscaling policy based on load balancing serving capacity.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Create a Managed Instance Group with a cluster autoscaler with a fixed minimum and a maximum number of instances.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a Managed Instance Group with an autohealing policy that attempts to recreate the crashed instance.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a Managed Instance Group with an autoscaling policy based on CPU utilization.",
        "status": "correct"
      }
    },
    "tag": [
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 2,
    "question_text": "An application development team believes their current logging tool will not meet their needs for their new cloud-based product. They want a better tool to capture errors and help them analyze their historical log data. You want to help them find a solution that meets their needs. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Direct them to download and install the Google StackDriver logging agent",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Send them a list of online resources about logging best practices",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Help them define their requirements and assess viable logging tools",
        "status": "correct"
      },
      "d": {
        "answer_text": "Help them upgrade their current tool to take advantage of any new features",
        "status": "incorrect"
      }
    },
    "tag": [
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 3,
    "question_text": "There is one application that has been utilizing a lot of bandwidth - sending out large packets. This particular app attempts to control the TCP window size so that it can maximize its own performance, to the detriment of other services running on the same VM. Which Linux tunable below would you adjust to set the maximum OS send buffer size for all connections?",
    "answers": {
      "a": {
        "answer_text": "Net.core.rmem_max",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Net.core.wmem_max",
        "status": "correct"
      },
      "c": {
        "answer_text": "net.ipv4.tcp_rmem",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "net.ipv4.tcp_wmem",
        "status": "incorrect"
      }
    },
    "tag": [
      "networking"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 4,
    "question_text": "A news feed web service has the following code running on Google App Engine. During peak load, users report that they can see news articles they already viewed. What is the most likely cause of this problem?",
    "answers": {
      "a": {
        "answer_text": "The session variable is local to just a single instance",
        "status": "correct"
      },
      "b": {
        "answer_text": "The session variable is being overwritten in Cloud Datastore",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "The URL of the API needs to be modified to prevent caching",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "The HTTP Expires header needs to be set to -1 stop caching",
        "status": "incorrect"
      }
    },
    "tag": [
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 5,
    "question_text": "Your company has recently acquired another company operating in Spain to expand your market in Europe. You have to integrate their legacy systems into your company's systems. Your company uses GCP as the primary cloud to manage your applications. Assuming your team is taking this opportunity to modernize parts of the legacy system, what is the best way to integrate this system into your company's systems?",
    "answers": {
      "a": {
        "answer_text": "Containerize the legacy applications using GKE and run them on the on-prem instances. Use Anthos for managing the applications",
        "status": "correct"
      },
      "b": {
        "answer_text": "Create snapshot of the legacy systems and run them on compute instances on GCP",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Containerize the legacy applications using GKE and run the GKE instances on GCP",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use transfer appliance to securely ship the data from on-prem systems to GCP",
        "status": "incorrect"
      }
    },
    "tag": [
      "migration",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 6,
    "question_text": "The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE application to the cloud. Which three practices should you recommend? (Choose three.)",
    "answers": {
      "a": {
        "answer_text": "Port the application code to run on Google App Engine",
        "status": "correct"
      },
      "b": {
        "answer_text": "Integrate Cloud Dataflow into the application to capture real-time metrics",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Instrument the application with a monitoring tool like Stackdriver Debugger",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Select an automation framework to reliably provision the cloud infrastructure",
        "status": "correct"
      },
      "e": {
        "answer_text": "Deploy a continuous integration tool with automated testing in a staging environment",
        "status": "correct"
      },
      "f": {
        "answer_text": "Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable",
        "status": "incorrect"
      }
    },
    "tag": [
      "migration",
      "devops"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 7,
    "question_text": "A start-up in the decentralized finance space provides portfolio management services for customers holding digital assets. In order to manage the digital assets they take custody of their customer's private keys which are very sensitive and prone to hacking. They plan to use the private keys of high value assets only when required and store them offline in their on-premise systems. They use GCP as their primary infrastructure and would like to know how to access and update the private keys securely only when required. What is the best way to achieve this? (Choose three)",
    "answers": {
      "a": {
        "answer_text": "Create a VM in a separate VPC and use it to temporarily store the private keys. Create a peering connection with the main VPC to update the or access the private keys.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Store the private keys on premise and copy them to the VM through cloud VPN.",
        "status": "correct"
      },
      "c": {
        "answer_text": "Copy the updated private keys back to on-prem system and delete the VM instance",
        "status": "correct"
      },
      "d": {
        "answer_text": "Connect the on-prem system to the main VPC through cloud VPN.",
        "status": "incorrect"
      }
    },
    "tag": [
      "security",
      "networking"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 8,
    "question_text": "Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your business analysts have experience only with using a SQL interface. How should you store the data to optimize it for ease of analysis?",
    "answers": {
      "a": {
        "answer_text": "Load data into Google BigQuery",
        "status": "correct"
      },
      "b": {
        "answer_text": "Insert data into Google Cloud SQL",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Put flat files into Google Cloud Storage",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Stream data into Google Cloud Datastore",
        "status": "incorrect"
      }
    },
    "tag": [
      "data-analytics",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 9,
    "question_text": "A development team has launched a vital application on a regional managed instance group that must have nearly zero downtime. To ensure high availability, you now have to configure a global load balancer to direct HTTP traffic across multiple zones. After configuring the backend then the frontend of the load balancer, you click Review and Finalize. Assuming you have configured the load balancer correctly, what backend settings do you see upon review?",
    "answers": {
      "a": {
        "answer_text": "The Backend service is web-app-backend. The Endpoint protocol is HTTP. The Health check is web-app-load-balancer-check. The Instance group is load-balancing-web-app-group.",
        "status": "correct"
      },
      "b": {
        "answer_text": "The Backend service is web-app-backend. The Endpoint protocol is TCP. The Health check is load-balancing-web-app-group. The Instance group is web-app-load-balancer-check.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "The Backend service is web-app-backend. The Endpoint protocol is HTTP. The Health check is load-balancing-web-app-group. The Instance group is web-app-load-balancer-check.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "The Backend service is web-app-backend. The Endpoint protocol is TCP. The Health check is web-app-load-balancer-check. The Instance group is load-balancing-web-app-group.",
        "status": "incorrect"
      }
    },
    "tag": [
      "networking"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 10,
    "question_text": "Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs. What should they do?",
    "answers": {
      "a": {
        "answer_text": "Configure a new load balancer for the new version of the API",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Reconfigure old clients to use a new endpoint for the new API",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Have the old API forward traffic to the new API based on the path",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use separate backend pools for each API path behind the load balancer",
        "status": "correct"
      }
    },
    "tag": [
      "networking"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 167,
    "question_text": "For this question, refer to the TerramEarth case study. TerramEarth has equipped unconnected trucks with servers and sensors to collet telemetry data. Next year they want to use the data to train machine learning models. They want to store this data in the cloud while reducing costs. What should they do?",
    "answers": {
      "a": {
        "answer_text": "Have the vehicle' computer compress the data in hourly snapshots, and store it in a Google Cloud storage (GCS) Nearline bucket.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Push the telemetry data in Real-time to a streaming dataflow job that compresses the data, and store it in Google BigQuery.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Have the vehicle's computer compress the data in hourly snapshots, a Store it in a GCS Coldline bucket.",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 168,
    "question_text": "For this question refer to the TerramEarth case study Operational parameters such as oil pressure are adjustable on each of TerramEarth's vehicles to increase their efficiency, depending on their environmental conditions. Your primary goal is to increase the operating efficiency of all 20 million cellular and unconnected vehicles in the field How can you accomplish this goal?",
    "answers": {
      "a": {
        "answer_text": "Have your engineers inspect the data for patterns, and then create an algorithm with rules that make operational adjustments automatically.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Capture all operating data, train machine learning models that identify ideal operations, and run locally to make operational adjustments automatically.",
        "status": "correct"
      },
      "c": {
        "answer_text": "Implement a Google Cloud Dataflow streaming job with a sliding window, and use Google Cloud Messaging (GCM) to make operational adjustments automatically.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Capture all operating data, train machine learning models that identify ideal operations, and host in Google Cloud Machine Learning (ML) Platform to make operational adjustments automatically.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "data-analytics"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 169,
    "question_text": "Your agricultural division is experimenting with fully autonomous vehicles. You want your architecture to promote strong security during vehicle operation. Which two architecture should you consider? Choose 2 answers:",
    "answers": {
      "a": {
        "answer_text": "Treat every micro service call between modules on the vehicle as untrusted.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Require IPv6 for connectivity to ensure a secure address space.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Use a trusted platform module (TPM) and verify firmware and binaries on boot.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Use a functional programming language to isolate code execution cycles.",
        "status": "incorrect"
      },
      "e": {
        "answer_text": "Use multiple connectivity subsystems for redundancy.",
        "status": "incorrect"
      },
      "f": {
        "answer_text": "Enclose the vehicle's drive electronics in a Faraday cage to isolate chips.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "security"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 170,
    "question_text": "For this question, refer to the JencoMart case study. The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model with separation of duties for administration between production and development resources. What Google domain and project structure should you recommend?",
    "answers": {
      "a": {
        "answer_text": "Create two G Suite accounts to manage users: one for development/test/staging and one for production. Each account should contain one project for every application.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Create two G Suite accounts to manage users: one with a single project for all development applications and one with a single project for all production applications.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a single G Suite account to manage users with each stage of each application in its own project.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a single G Suite account to manage users with one project for the development/test/staging environment and one project for the production environment.",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "security"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 171,
    "question_text": "For this question, refer to the JencoMart case study. The migration of JencoMart's application to Google Cloud Platform (GCP) is progressing too slowly. The infrastructure is shown in the diagram. You want to maximize throughput. What are three potential bottlenecks? (Choose 3 answers.)",
    "answers": {
      "a": {
        "answer_text": "A single VPN tunnel, which limits throughput",
        "status": "correct"
      },
      "b": {
        "answer_text": "A tier of Google Cloud Storage that is not suited for this task",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "A copy command that is not suited to operate over long distances",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Fewer virtual machines (VMs) in GCP than on-premises machines",
        "status": "correct"
      },
      "e": {
        "answer_text": "A separate storage layer outside the VMs, which is not suited for this task",
        "status": "incorrect"
      },
      "f": {
        "answer_text": "Complicated internet connectivity between the on-premises infrastructure and GCP",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "migration"
    ],
    "explanation": "",
    "hint": "",
    "score": -1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 172,
    "question_text": "For this question, refer to the JencoMart case study A few days after JencoMart migrates the user credentials database to Google Cloud Platform and shuts down the old server, the new database server stops responding to SSH connections. It is still serving database requests to the application servers correctly. What three steps should you take to diagnose the problem? Choose 3 answers",
    "answers": {
      "a": {
        "answer_text": "Delete the virtual machine (VM) and disks and create a new one.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Delete the instance, attach the disk to a new VM, and investigate.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Take a snapshot of the disk and connect to a new machine to investigate.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Check inbound firewall rules for the network the machine is connected to.",
        "status": "correct"
      },
      "e": {
        "answer_text": "Connect the machine to another network with very simple firewall rules and investigate.",
        "status": "incorrect"
      },
      "f": {
        "answer_text": "Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate.",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 173,
    "question_text": "For this question, refer to the JencoMart case study. JencoMart wants to move their User Profiles database to Google Cloud Platform. Which Google Database should they use?",
    "answers": {
      "a": {
        "answer_text": "Cloud Spanner",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Google BigQuery",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Google Cloud SQL",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Google Cloud Datastore",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 174,
    "question_text": "For this question, refer to the JencoMart case study. JencoMart has decided to migrate user profile storage to Google Cloud Datastore and the application servers to Google Compute Engine (GCE). During the migration, the existing infrastructure will need access to Datastore to upload the data. What service account key-management strategy should you recommend?",
    "answers": {
      "a": {
        "answer_text": "Provision service account keys for the on-premises infrastructure and for the GCE virtual machines (VMs).",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Authenticate the on-premises infrastructure with a user account and provision service account keys for the VMs.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs",
        "status": "correct"
      },
      "d": {
        "answer_text": "Deploy a custom authentication service on GCE/Google Container Engine (GKE) for the on- premises infrastructure and use GCP managed keys for the VMs.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "security"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 175,
    "question_text": "For this question, refer to the JencoMart case study. JencoMart has built a version of their application on Google Cloud Platform that serves traffic to Asia. You want to measure success against their business and technical goals. Which metrics should you track?",
    "answers": {
      "a": {
        "answer_text": "Error rates for requests from Asia",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Latency difference between US and Asia",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Total visits, error rates, and latency from Asia",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Total visits and average latency for users in Asia",
        "status": "correct"
      },
      "e": {
        "answer_text": "The number of character sets present in the database",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 176,
    "question_text": "For this question, refer to the Dress4Win case study. At Dress4Win, an operations engineer wants to create a tow-cost solution to remotely archive copies of database backup files. The database files are compressed tar files stored in their current data center. How should he proceed?",
    "answers": {
      "a": {
        "answer_text": "Create a cron script using gsutil to copy the files to a Coldline Storage bucket.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Create a cron script using gsutil to copy the files to a Regional Storage bucket.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a Cloud Storage Transfer Service Job to copy the files to a Coldline Storage bucket.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a Cloud Storage Transfer Service job to copy the files to a Regional Storage bucket.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 177,
    "question_text": "For this question, refer to the Dress4Win case study. Dress4Win has asked you to recommend machine types they should deploy their application servers to. How should you proceed?",
    "answers": {
      "a": {
        "answer_text": "Perform a mapping of the on-premises physical hardware cores and RAM to the nearest machine types in the cloud.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Recommend that Dress4Win deploy application servers to machine types that offer the highest RAM to CPU ratio available.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Recommend that Dress4Win deploy into production with the smallest instances available, monitor them over time, and scale the machine type up until the desired performance is reached.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Identify the number of virtual cores and RAM associated with the application server virtual machines align them to a custom machine type in the cloud, monitor performance, and scale the machine types up until the desired performance is reached.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 178,
    "question_text": "For this question, refer to the Dress4Win case study. Dress4Win has asked you for advice on how to migrate their on-premises MySQL deployment to the cloud. They want to minimize downtime and performance impact to their on-premises solution during the migration. Which approach should you recommend?",
    "answers": {
      "a": {
        "answer_text": "Create a dump of the on-premises MySQL master server, and then shut it down, upload it to the cloud environment, and load into a new MySQL cluster.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Setup a MySQL replica server/slave in the cloud environment, and configure it for asynchronous replication from the MySQL master server on-premises until cutover.",
        "status": "correct"
      },
      "c": {
        "answer_text": "Create a new MySQL cluster in the cloud, configure applications to begin writing to both on-premises and cloud MySQL masters, and destroy the original cluster at cutover.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a dump of the MySQL replica server into the cloud environment, load it into: Google Cloud Datastore, and configure applications to read/write to Cloud Datastore at cutover.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "migration"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 179,
    "question_text": "For this question, refer to the Dress4Win case study. Dress4Win has configured a new uptime check with Google Stackdriver for several of their legacy services. The Stackdriver dashboard is not reporting the services as healthy. What should they do?",
    "answers": {
      "a": {
        "answer_text": "Install the Stackdriver agent on all of the legacy web servers.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "In the Cloud Platform Console download the list of the uptime servers' IP addresses and create an inbound firewall rule",
        "status": "correct"
      },
      "c": {
        "answer_text": "Configure their load balancer to pass through the User-Agent HTTP header when the value matches GoogleStackdriverMonitoring-UptimeChecks (https://cloud.google.com/monitoring)",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Configure their legacy web servers to allow requests that contain user-Agent HTTP header when the value matches GoogleStackdriverMonitoring-- UptimeChecks (https://cloud.google.com/monitoring)",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 180,
    "question_text": "For this question, refer to the Dress4Win case study. You want to ensure Dress4Win's sales and tax records remain available for infrequent viewing by auditors for at least 10 years. Cost optimization is your top priority. Which cloud services should you choose?",
    "answers": {
      "a": {
        "answer_text": "Google Cloud Storage Coldline to store the data, and gsutil to access the data.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Google Cloud Storage Nearline to store the data, and gsutil to access the data.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Google Bigtabte with US or EU as location to store the data, and gcloud to access the data.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "BigQuery to store the data, and a web server cluster in a managed instance group to access the data. Google Cloud SQL mirrored across two distinct regions to store the data, and a Redis cluster in a managed instance group to access the data.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 181,
    "question_text": "For this question, refer to the Dress4Win case study. Dress4Win has end-to-end tests covering 100% of their endpoints. They want to ensure that the move to the cloud does not introduce any new bugs. Which additional testing methods should the developers employ to prevent an outage?",
    "answers": {
      "a": {
        "answer_text": "They should enable Google Stackdriver Debugger on the application code to show errors in the code.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "They should add additional unit tests and production scale load tests on their cloud staging environment.",
        "status": "correct"
      },
      "c": {
        "answer_text": "They should run the end-to-end tests in the cloud staging environment to determine if the code is working as intended.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "They should add canary tests so developers can measure how much of an impact the new release causes to latency.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "devops"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 182,
    "question_text": "For this question, refer to the Dress4Win case study. As part of their new application experience, Dress4Wm allows customers to upload images of themselves. The customer has exclusive control over who may view these images. Customers should be able to upload images with minimal latency and also be shown their images quickly on the main application page when they log in. Which configuration should Dress4Win use?",
    "answers": {
      "a": {
        "answer_text": "Store image files in a Google Cloud Storage bucket. Use Google Cloud Datastore to maintain metadata that maps each customer's ID and their image files.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Store image files in a Google Cloud Storage bucket. Add custom metadata to the uploaded images in Cloud Storage that contains the customer's unique ID.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Use a distributed file system to store customers' images. As storage needs increase, add more persistent disks and/or nodes. Assign each customer a unique ID, which sets each file's owner attribute, ensuring privacy of images.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use a distributed file system to store customers' images. As storage needs increase, add more persistent disks and/or nodes. Use a Google Cloud SQL database to maintain metadata that maps each customer's ID to their image files.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 183,
    "question_text": "For this question, refer to the Dress4Win case study. The Dress4Win security team has disabled external SSH access into production virtual machines (VMs) on Google Cloud Platform (GCP). The operations team needs to remotely manage the VMs, build and push Docker containers, and manage Google Cloud Storage objects. What can they do?",
    "answers": {
      "a": {
        "answer_text": "Grant the operations engineers access to use Google Cloud Shell.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Configure a VPN connection to GCP to allow SSH access to the cloud VMs.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Develop a new access request process that grants temporary SSH access to cloud VMs when an operations engineer needs to perform a task.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Have the development team build an API service that allows the operations team to execute specific remote procedure calls to accomplish their tasks.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "security"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 184,
    "question_text": "For this question, refer to the Dress4Win case study. Dress4Win would like to become familiar with deploying applications to the cloud by successfully deploying some applications quickly, as is. They have asked for your recommendation. What should you advise?",
    "answers": {
      "a": {
        "answer_text": "Identify self-contained applications with external dependencies as a first move to the cloud.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Identify enterprise applications with internal dependencies and recommend these as a first move to the cloud.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Suggest moving their in-house databases to the cloud and continue serving requests to on- premise applications.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Recommend moving their message queuing servers to the cloud and continue handling requests to on-premise applications.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "migration"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 185,
    "question_text": "For this question, refer to the Dress4Win case study. As part of Dress4Win's plans to migrate to the cloud, they want to be able to set up a managed logging and monitoring system so they can handle spikes in their traffic load. They want to ensure that: The infrastructure can be notified when it needs to scale up and down to handle the ebb and flow of usage throughout the day, Their administrators are notified automatically when their application reports errors, They can filter their aggregated logs down in order to debug one piece of the application across many hosts. Which Google StackDriver features should they use?",
    "answers": {
      "a": {
        "answer_text": "Logging, Alerts, Insights, Debug",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Monitoring, Trace, Debug, Logging",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Monitoring, Logging, Alerts, Error Reporting",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Monitoring, Logging, Debug, Error Report",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 187,
    "question_text": "The current Dress4win system architecture has high latency to some customers because it is located in one data center. As of a future evaluation and optimizing for performance in the cloud, Dresss4win wants to distribute it's system architecture to multiple locations when Google cloud platform. Which approach should they use?",
    "answers": {
      "a": {
        "answer_text": "Use regional managed instance groups and a global load balancer to increase performance because the regional managed instance group can grow instances in each region separately based on traffic.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines managed by your operations team.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Use regional managed instance groups and a global load balancer to increase reliability by providing automatic failover between zones in different regions.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines as part of a separate managed instance groups.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 188,
    "question_text": "For this question, refer to the Dress4Win case study. Dress4Win is expected to grow to 10 times its size in 1 year with a corresponding growth in data and traffic that mirrors the existing patterns of usage. The CIO has set the target of migrating production infrastructure to the cloud within the next 6 months. How will you configure the solution to scale for this growth without making major application changes and still maximize the ROI?",
    "answers": {
      "a": {
        "answer_text": "Migrate the web application layer to App Engine, and MySQL to Cloud Datastore, and NAS to Cloud Storage. Deploy RabbitMQ, and deploy Hadoop servers using Deployment Manager.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Migrate RabbitMQ to Cloud Pub/Sub, Hadoop to BigQuery, and NAS to Compute Engine with Persistent Disk storage. Deploy Tomcat, and deploy Nginx using Deployment Manager.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Implement managed instance groups for Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub, Hadoop to Cloud Dataproc, and NAS to Compute Engine with Persistent Disk storage.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Implement managed instance groups for the Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub, Hadoop to Cloud Dataproc, and NAS to Cloud Storage.",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "migration"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 189,
    "question_text": "For this question, refer to the Dress4Win case study. Considering the given business requirements, how would you automate the deployment of web and transactional data layers?",
    "answers": {
      "a": {
        "answer_text": "Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine. Deploy a Cloud SQL server to replace MySQL. Deploy Jenkins using Cloud Deployment Manager.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Deploy Nginx and Tomcat using Cloud Launcher. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute Engine using Cloud Deployment Manager scripts.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Migrate Nginx and Tomcat to App Engine. Deploy a Cloud Datastore server to replace the MySQL server in a high- availability configuration. Deploy Jenkins to Compute Engine using Cloud Launcher.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Migrate Nginx and Tomcat to App Engine. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute Engine using Cloud Launcher.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "devops"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 190,
    "question_text": "For this question, refer to the Dress4Win case study. Which of the compute services should be migrated as is and would still be an optimized architecture for performance in the cloud?",
    "answers": {
      "a": {
        "answer_text": "Web applications deployed using App Engine standard environment",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "RabbitMQ deployed using an unmanaged instance group",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Hadoop/Spark deployed using Cloud Dataproc Regional in High Availability mode",
        "status": "correct"
      },
      "d": {
        "answer_text": "Jenkins, monitoring, bastion hosts, security scanners services deployed on custom machine types",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 191,
    "question_text": "For this question, refer to the Dress4Win case study. To be legally compliant during an audit, Dress4Win must be able to give insights in all administrative actions that modify the configuration or metadata of resources on Google Cloud. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Use Stackdriver Trace to create a trace list analysis.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Use Stackdriver Monitoring to create a dashboard on the project's activity.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Enable Cloud Identity-Aware Proxy in all projects, and add the group of Administrators as a member.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use the Activity page in the GCP Console and Stackdriver Logging to provide the required insight.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 192,
    "question_text": "For this question, refer to the Dress4Win case study. You are responsible for the security of data stored in Cloud Storage for your company, Dress4Win. You have already created a set of Google Groups and assigned the appropriate users to those groups. You should use Google best practices and implement the simplest design to meet the requirements. Considering Dress4Win's business and technical requirements, what should you do?",
    "answers": {
      "a": {
        "answer_text": "Assign custom IAM roles to the Google Groups you created in order to enforce security requirements. Encrypt data with a customer-supplied encryption key when storing files in Cloud Storage.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Assign custom IAM roles to the Google Groups you created in order to enforce security requirements. Enable default storage encryption before storing files in Cloud Storage.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Assign predefined IAM roles to the Google Groups you created in order to enforce security requirements. Utilize Google's default encryption at rest when storing files in Cloud Storage.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Assign predefined IAM roles to the Google Groups you created in order to enforce security requirements. Ensure that the default Cloud KMS key is set before storing files in Cloud Storage.",
        "status": "correct"
      }
    },
    "tag": [
      "case-study",
      "security"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 193,
    "question_text": "For this question, refer to the Dress4Win case study. You want to ensure that your on-premises architecture meets business requirements before you migrate your solution. What change in the on-premises architecture should you make?",
    "answers": {
      "a": {
        "answer_text": "Replace RabbitMQ with Google Pub/Sub.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Downgrade MySQL to v5.7, which is supported by Cloud SQL for MySQL.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Resize compute resources to match predefined Compute Engine machine types.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Containerize the micro services and host them in Google Kubernetes Engine.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "migration"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 194,
    "question_text": "For this question, refer to the TerramEarth case study. To be compliant with European GDPR regulation, TerramEarth is required to delete data generated from its European customers after a period of 36 months when it contains personal data In the new architecture, this data will be stored in both Cloud Storage and BigQuery. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to create a SetStorageClass to NONE action when with an Age condition of 36 months.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Create a BigQuery time-partitioned table for the European data, and set the partition period to 36 months. For Cloud Storage, use gsutil to create a SetStorageClass to NONE action with an Age condition of 36 months.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 195,
    "question_text": "For this question, refer to the TerramEarth case study. TerramEarth has decided to store data files in Cloud Storage. You need to configure Cloud Storage lifecycle rule to store 1 year of data and minimize file storage cost. Which two actions should you take?",
    "answers": {
      "a": {
        "answer_text": "Create a Cloud Storage lifecycle rule with Age: \"30\", Storage Class: \"Standard\", and Action: \"Set to Coldline\", and create a second GCS life-cycle rule with Age: \"365\", Storage Class: \"Coldline\", and Action: \"Delete\".",
        "status": "correct"
      },
      "b": {
        "answer_text": "Create a Cloud Storage lifecycle rule with Age: \"30\", Storage Class: \"Coldline\", and Action: \"Set to Nearline\", and create a second GCS life-cycle rule with Age: \"91\", Storage Class: \"Coldline\", and Action: \"Set to Nearline\".",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a Cloud Storage lifecycle rule with Age: \"90\", Storage Class: \"Standard\", and Action: \"Set to Nearline\", and create a second GCS life-cycle rule with Age: \"91\", Storage Class: \"Nearline\", and Action: \"Set to Coldline\".",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a Cloud Storage lifecycle rule with Age: \"30\", Storage Class: \"Standard\", and Action: \"Set to Coldline\", and create a second GCS life-cycle rule with Age: \"365\", Storage Class: \"Nearline\", and Action: \"Delete\".",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "cost-optimization"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 196,
    "question_text": "For this question, refer to the TerramEarth case study. You need to implement a reliable, scalable GCP solution for the data warehouse for your company, TerramEarth. Considering the TerramEarth business and technical requirements, what should you do?",
    "answers": {
      "a": {
        "answer_text": "Replace the existing data warehouse with BigQuery. Use table partitioning.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Replace the existing data warehouse with a Compute Engine instance with 96 CPUs.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Replace the existing data warehouse with BigQuery. Use federated data sources.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Replace the existing data warehouse with a Compute Engine instance with 96 CPUs. Add an additional Compute Engine pre-emptible instance with 32 CPUs.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "data-analytics"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 197,
    "question_text": "For this question, refer to the TerramEarth case study. A new architecture that writes all incoming data to BigQuery has been introduced. You notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Set up a streaming Cloud Dataflow job, receiving data by the ingestion process. Clean the data in a Cloud Dataflow pipeline.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Create a Cloud Function that reads data from BigQuery and cleans it. Trigger it. Trigger the Cloud Function from a Compute Engine instance.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a SQL statement on the data in BigQuery, and save it as a view. Run the view daily, and save the result to a new table.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "data-analytics"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 198,
    "question_text": "For this question, refer to the TerramEarth case study. Considering the technical requirements, how should you reduce the unplanned vehicle downtime in GCP?",
    "answers": {
      "a": {
        "answer_text": "Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Use BigQuery as the data warehouse. Connect all vehicles to the network and upload gzip files to a Multi-Regional Cloud Storage bucket using gcloud. Use Google Data Studio for analysis and reporting.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Use Cloud Dataproc Hive as the data warehouse. Upload gzip files to a MultiRegional Cloud Storage bucket. Upload this data into BigQuery using gcloud. Use Google data Studio for analysis and reporting.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use Cloud Dataproc Hive as the data warehouse. Directly stream data into prtitioned Hive tables. Use Pig scripts to analyze data.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "data-analytics"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 199,
    "question_text": "For this question, refer to the TerramEarth case study. You are asked to design a new architecture for the ingestion of the data of the 200,000 vehicles that are connected to a cellular network. You want to follow Google-recommended practices. Considering the technical requirements, which components should you use for the ingestion of the data?",
    "answers": {
      "a": {
        "answer_text": "Google Kubernetes Engine with an SSL Ingress",
        "status": "correct"
      },
      "b": {
        "answer_text": "Cloud IoT Core with public/private key pairs",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Compute Engine with project-wide SSH keys",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Compute Engine with specific SSH keys",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": -1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 200,
    "question_text": "For this question, refer to the TerramEarth case study. TerramEarth has about 1 petabyte (PB) of vehicle testing data in a private data center. You want to move the data to Cloud Storage for your machine learning team. Currently, a 1-Gbps interconnect link is available for you. The machine learning team wants to start using the data in a month. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Request Transfer Appliances from Google Cloud, export the data to appliances, and return the appliances to Google Cloud.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Configure the Storage Transfer service from Google Cloud to send the data from your data center to Cloud Storage",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Make sure there are no other users consuming the 1 Gbps link, and use multi-thread transfer to upload the data to Cloud Storage.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Export files to an encrypted USB device, send the device to Google Cloud, and request an import of the data to Cloud Storage",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "migration"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 201,
    "question_text": "Your company wants you to build a highly reliable web application with a few public APIs as the backend. You don't expect a lot of user traffic, but traffic could spike occasionally. You want to leverage Cloud Load Balancing, and the solution must be cost-effective for users. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Store static content such as HTML and images in Cloud CDN. Host the APIs on App Engine and store the user data in Cloud SQL.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Store static content such as HTML and images in a Cloud Storage bucket. Host the APIs on a zonal Google Kubernetes Engine cluster with worker nodes in multiple zones, and save the user data in Cloud Spanner.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Store static content such as HTML and images in Cloud CDN. Use Cloud Run to host the APIs and save the user data in Cloud SQL.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Store static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save the user data in Firestore.",
        "status": "correct"
      }
    },
    "tag": [
      "compute",
      "cost-optimization"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 202,
    "question_text": "You are developing your microservices application on Google Kubernetes Engine. During testing, you want to validate the behavior of your application in case a specific microservice should suddenly crash. What should you do?",
    "answers": {
      "a": {
        "answer_text": "Add a taint to one of the nodes of the Kubernetes cluster. For the specific microservice, configure a pod anti-affinity label that has the name of the tainted node as a value.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Use Istio's fault injection on the particular microservice whose faulty behavior you want to simulate.",
        "status": "correct"
      },
      "c": {
        "answer_text": "Destroy one of the nodes of the Kubernetes cluster to observe the behavior.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Configure Istio's traffic management features to steer the traffic away from a crashing microservice.",
        "status": "incorrect"
      }
    },
    "tag": [
      "compute",
      "devops"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 203,
    "question_text": "You are implementing the infrastructure for a web service on Google Cloud. The web service needs to receive and store the data from 500,000 requests per second. The data will be queried later in real time, based on exact matches of a known set of attributes. There will be periods where the web service will not receive any requests. The business wants to keep costs low. Which web service platform and database should you use for the application?",
    "answers": {
      "a": {
        "answer_text": "Cloud Run and BigQuery",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Cloud Run and Cloud Bigtable",
        "status": "correct"
      },
      "c": {
        "answer_text": "A Compute Engine autoscaling managed instance group and BigQuery",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "A Compute Engine autoscaling managed instance group and Cloud Bigtable",
        "status": "incorrect"
      }
    },
    "tag": [
      "compute",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 204,
    "question_text": "You have an application that runs in Google Kubernetes Engine (GKE). Over the last 2 weeks, customers have reported that a specific part of the application returns errors very frequently. You currently have no logging or monitoring solution enabled on your GKE cluster. You want to diagnose the problem, but you have not been able to replicate the issue. You want to cause minimal disruption to the application. What should you do?",
    "answers": {
      "a": {
        "answer_text": "1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected Pods.",
        "status": "correct"
      },
      "b": {
        "answer_text": "1. Create a new GKE cluster with Cloud Operations for GKE enabled. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Use the GKE Monitoring dashboard to investigate logs from affected Pods.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "1. Update your GKE cluster to use Cloud Operations for GKE, and deploy Prometheus. 2. Set an alert to trigger whenever the application returns an error.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an error.",
        "status": "incorrect"
      }
    },
    "tag": [
      "monitoring",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 205,
    "question_text": "You are updating a CSV file stored in Cloud Storage and want to append the data to an existing BigQuery table with the same schema using a bq load command. After executing the command, the job failed. The answers suggest the error is because a field expected to be an integer has a NULL value in the CSV. How should you solve this issue to append the CSV data to the existing BigQuery table?",
    "answers": {
      "a": {
        "answer_text": "bq load default setup cannot load more than 1000 rows at once, add the option –limit_rows=false to the command above and re execute it.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "bq load is failing most likely because score_value field is expected to be int but has NULL value in the CSV file. add option –bad_records=true to ignore the nulls –replace=false for appending only.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "bq load is failing most likely because score_value field is expected to be int but has NULL value in the CSV file. add option –null_marker to mark nulls and –replace for appending only.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "bq load is failing most likely because score_value field is expected to be int but has NULL value in the CSV file. add option –null_marker to mark nulls and –replace=false for appending only.",
        "status": "correct"
      }
    },
    "tag": [
      "data-analytics",
      "storage"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 206,
    "question_text": "An analytics consulting firm is considering using Microsoft Active Directory in Google Cloud to centrally manage their users and applications. It is important that the server is highly available and fault tolerant as the employees use it globally to connect to different applications. As a GCP cloud architect what solution will you recommend to the firm?",
    "answers": {
      "a": {
        "answer_text": "Create a custom mode VPC network with 2 subnets spanning 2 zones. Create Windows Server virtual instances and enable Active Directory Domain Services. Configure a new domain with Active Directory. Join the new Windows Server instances to the new domain and configure firewall rules to allow traffic to the virtual machines.",
        "status": "correct"
      },
      "b": {
        "answer_text": "Create a VPC with one subnet spanning 2 zones. Create Windows Server virtual instances and enable Active Directory Domain Services. Configure a new domain with Active Directory. Join the new Windows Server instances to the new domain and configure firewall rules to allow traffic to the virtual machines.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a custom mode VPC network with 2 subnets spanning 2 zones. Create Windows server virtual instances and enable Active Directory Domain Services. Create a load balancer with auto scaling group policy to run two virtual machines.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a VPC with one subnet spanning 2 zones. Create Windows server virtual instances and enable Active Directory Domain Services. Create a load balancer with auto scaling group policy to run two virtual machines.",
        "status": "incorrect"
      }
    },
    "tag": [
      "security",
      "networking"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 207,
    "question_text": "You are working in an Infrastructure team where you are using Google Cloud Platform as your Cloud Provider. Your boss has asked you to configure a compute engine to host an application that needs access to a recently created Storage Bucket. Your boss tells you to use a new Service Account to do this. How would you achieve the given requirement in the most reliable way?",
    "answers": {
      "a": {
        "answer_text": "Create a new Service Account with a role as Storage Admin. While configuring Compute Engine, Choose the newly created Service Account and Set the scope to Storage API. Set of path of service account keys in your application.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Create a Service Account with role as Storage Admin. While configuring Compute Engine, Choose the newly created Service Account and Set the scope to Storage API. No need to Set the path of service account keys in your application.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a Service Account with role as Storage Admin. While configuring Compute Engine, Choose the newly created Service Account. Set of path of service account keys in your application.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Create a Service Account with role as Storage Admin. While configuring Compute Engine, Choose the newly created Service Account. No need to Set the path of service account keys in your application.",
        "status": "correct"
      }
    },
    "tag": [
      "security",
      "compute"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 208,
    "question_text": "A hedge fund firm is using google cloud infrastructure for their compute and storage requirements. In order to comply with regulations, the company should store all the files available for audit by the authorities. All the files up to past 3 years should be available for audit before deletion. The employees are aware of the regulatory requirements, but sometimes they might delete the files accidentally. The company wants to avoid such accidents as they attract fines from the authorities. How should they manage the storage to be compliant to the regulation?",
    "answers": {
      "a": {
        "answer_text": "Use cloud storage for storing data. Enable multi region buckets and back-up data to a nearline storage. Set object lifecycle policy to delete objects that are more than 3 years old.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Use cloud storage for storing data. Add retention policy to the buckets with retention period of 3 years.",
        "status": "correct"
      },
      "c": {
        "answer_text": "Use cloud storage for storing data. Enable multi region buckets and back-up data to a coldline storage. Set object lifecycle policy to delete objects that are more than 3 years old.",
        "status": "incorrect"
      },
      "d": {
        "answer_text": "Use cloud storage for storing data. Enable object versioning for the buckets. Use object lifecycle policy to delete objects that are more than 3 years old.",
        "status": "incorrect"
      }
    },
    "tag": [
      "storage",
      "security"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 209,
    "question_text": "For this question, refer to the TerramEarth case study. TerramEarth has a legacy web application that you cannot migrate to cloud. However, you still want to build a cloud-native way to monitor the application. If the application goes down, you want the URL to point to a 'Site is unavailable' page as soon as possible. You also want your Ops team to receive a notification for the issue. You need to build a reliable solution for minimum cost What should you do?",
    "answers": {
      "a": {
        "answer_text": "Create a scheduled job in Cloud Run to invoke a container every minute. The container will check the application URL If the application is down, switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
        "status": "incorrect"
      },
      "b": {
        "answer_text": "Create a cron job on a Compute Engine VM that runs every minute. The cron job invokes a Python program to check the application URL If the application is down, switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
        "status": "incorrect"
      },
      "c": {
        "answer_text": "Create a Cloud Monitoring uptime check to validate the application URL If it fails, put a message in a Pub/Sub queue that triggers a Cloud Function to switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
        "status": "correct"
      },
      "d": {
        "answer_text": "Use Cloud Error Reporting to check the application URL If the application is down, switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
        "status": "incorrect"
      }
    },
    "tag": [
      "case-study",
      "monitoring"
    ],
    "explanation": "",
    "hint": "",
    "score": 0,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },





  
  {
    "question_number": 210,
    "question_text": "This is test question nr.1",
    "answers": {
      "a": {
        "answer_text": "correct option",
        "status": "correct"
      },
      "b": {
        "answer_text": "incorrect option",
        "status": "incorrect"
      }
    },
    "tag": [
      "test"
    ],
    "explanation": "",
    "hint": "",
    "score": 1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 211,
    "question_text": "This is test question nr.2",
    "answers": {
      "a": {
        "answer_text": "correct option",
        "status": "correct"
      },
      "b": {
        "answer_text": "incorrect option",
        "status": "incorrect"
      }
    },
    "tag": [
      "test"
    ],
    "explanation": "",
    "hint": "",
    "score": 1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 212,
    "question_text": "This is test question nr.3",
    "answers": {
      "a": {
        "answer_text": "correct option",
        "status": "correct"
      },
      "b": {
        "answer_text": "incorrect option",
        "status": "incorrect"
      }
    },
    "tag": [
      "test"
    ],
    "explanation": "",
    "hint": "",
    "score": 1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 213,
    "question_text": "This is test question nr.4",
    "answers": {
      "a": {
        "answer_text": "correct option",
        "status": "correct"
      },
      "b": {
        "answer_text": "incorrect option",
        "status": "incorrect"
      }
    },
    "tag": [
      "test"
    ],
    "explanation": "",
    "hint": "",
    "score": 1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  },
  {
    "question_number": 214,
    "question_text": "This is test question nr.5",
    "answers": {
      "a": {
        "answer_text": "correct option",
        "status": "correct"
      },
      "b": {
        "answer_text": "incorrect option",
        "status": "incorrect"
      }
    },
    "tag": [
      "test"
    ],
    "explanation": "",
    "hint": "",
    "score": 1,
    "starred": false,
    "note": "",
    "placeholder_1": "",
    "placeholder_2": "",
    "placeholder_3": ""
  }
]
